{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Porter Stemming Algorithm\n",
        "This is the Porter stemming algorithm, ported to Python from the\n",
        "version coded up in ANSI C by the author. It may be be regarded\n",
        "as canonical, in that it follows the algorithm presented in\n",
        "\n",
        "Porter, 1980, An algorithm for suffix stripping, Program, Vol. 14,\n",
        "no. 3, pp 130-137,\n",
        "\n",
        "only differing from it at the points maked --DEPARTURE-- below.\n",
        "\n",
        "See also http://www.tartarus.org/~martin/PorterStemmer\n",
        "\n",
        "The algorithm as described in the paper could be exactly replicated\n",
        "by adjusting the points of DEPARTURE, but this is barely necessary,\n",
        "because (a) the points of DEPARTURE are definitely improvements, and\n",
        "(b) no encoding of the Porter stemmer I have seen is anything like\n",
        "as exact as this version, even with the points of DEPARTURE!\n",
        "\n",
        "Vivake Gupta (v@nano.com)\n",
        "\n",
        "Release 1: January 2001\n",
        "\n",
        "Further adjustments by Santiago Bruno (bananabruno@gmail.com)\n",
        "to allow word input not restricted to one word per line, leading\n",
        "to:\n",
        "\n",
        "release 2: July 2008\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "\n",
        "class PorterStemmer:\n",
        "    def __init__(self):\n",
        "        \"\"\"The main part of the stemming algorithm starts here.\n",
        "        b is a buffer holding a word to be stemmed. The letters are in b[k0],\n",
        "        b[k0+1] ... ending at b[k]. In fact k0 = 0 in this demo program. k is\n",
        "        readjusted downwards as the stemming progresses. Zero termination is\n",
        "        not in fact used in the algorithm.\n",
        "\n",
        "        Note that only lower case sequences are stemmed. Forcing to lower case\n",
        "        should be done before stem(...) is called.\n",
        "        \"\"\"\n",
        "\n",
        "        self.b = \"\"  # buffer for word to be stemmed\n",
        "        self.k = 0\n",
        "        self.k0 = 0\n",
        "        self.j = 0   # j is a general offset into the string\n",
        "\n",
        "    def cons(self, i):\n",
        "        \"\"\"cons(i) is TRUE <=> b[i] is a consonant.\"\"\"\n",
        "        if self.b[i] == 'a' or self.b[i] == 'e' or self.b[i] == 'i' or self.b[i] == 'o' or self.b[i] == 'u':\n",
        "            return 0\n",
        "        if self.b[i] == 'y':\n",
        "            if i == self.k0:\n",
        "                return 1\n",
        "            else:\n",
        "                return (not self.cons(i - 1))\n",
        "        return 1\n",
        "\n",
        "    def m(self):\n",
        "        \"\"\"m() measures the number of consonant sequences between k0 and j.\n",
        "        if c is a consonant sequence and v a vowel sequence, and <..>\n",
        "        indicates arbitrary presence,\n",
        "\n",
        "           <c><v>       gives 0\n",
        "           <c>vc<v>     gives 1\n",
        "           <c>vcvc<v>   gives 2\n",
        "           <c>vcvcvc<v> gives 3\n",
        "           ....\n",
        "        \"\"\"\n",
        "        n = 0\n",
        "        i = self.k0\n",
        "        while 1:\n",
        "            if i > self.j:\n",
        "                return n\n",
        "            if not self.cons(i):\n",
        "                break\n",
        "            i = i + 1\n",
        "        i = i + 1\n",
        "        while 1:\n",
        "            while 1:\n",
        "                if i > self.j:\n",
        "                    return n\n",
        "                if self.cons(i):\n",
        "                    break\n",
        "                i = i + 1\n",
        "            i = i + 1\n",
        "            n = n + 1\n",
        "            while 1:\n",
        "                if i > self.j:\n",
        "                    return n\n",
        "                if not self.cons(i):\n",
        "                    break\n",
        "                i = i + 1\n",
        "            i = i + 1\n",
        "\n",
        "    def vowelinstem(self):\n",
        "        \"\"\"vowelinstem() is TRUE <=> k0,...j contains a vowel\"\"\"\n",
        "        for i in range(self.k0, self.j + 1):\n",
        "            if not self.cons(i):\n",
        "                return 1\n",
        "        return 0\n",
        "\n",
        "    def doublec(self, j):\n",
        "        \"\"\"doublec(j) is TRUE <=> j,(j-1) contain a double consonant.\"\"\"\n",
        "        if j < (self.k0 + 1):\n",
        "            return 0\n",
        "        if (self.b[j] != self.b[j-1]):\n",
        "            return 0\n",
        "        return self.cons(j)\n",
        "\n",
        "    def cvc(self, i):\n",
        "        \"\"\"cvc(i) is TRUE <=> i-2,i-1,i has the form consonant - vowel - consonant\n",
        "        and also if the second c is not w,x or y. this is used when trying to\n",
        "        restore an e at the end of a short  e.g.\n",
        "\n",
        "           cav(e), lov(e), hop(e), crim(e), but\n",
        "           snow, box, tray.\n",
        "        \"\"\"\n",
        "        if i < (self.k0 + 2) or not self.cons(i) or self.cons(i-1) or not self.cons(i-2):\n",
        "            return 0\n",
        "        ch = self.b[i]\n",
        "        if ch == 'w' or ch == 'x' or ch == 'y':\n",
        "            return 0\n",
        "        return 1\n",
        "\n",
        "    def ends(self, s):\n",
        "        \"\"\"ends(s) is TRUE <=> k0,...k ends with the string s.\"\"\"\n",
        "        length = len(s)\n",
        "        if s[length - 1] != self.b[self.k]: # tiny speed-up\n",
        "            return 0\n",
        "        if length > (self.k - self.k0 + 1):\n",
        "            return 0\n",
        "        if self.b[self.k-length+1:self.k+1] != s:\n",
        "            return 0\n",
        "        self.j = self.k - length\n",
        "        return 1\n",
        "\n",
        "    def setto(self, s):\n",
        "        \"\"\"setto(s) sets (j+1),...k to the characters in the string s, readjusting k.\"\"\"\n",
        "        length = len(s)\n",
        "        self.b = self.b[:self.j+1] + s + self.b[self.j+length+1:]\n",
        "        self.k = self.j + length\n",
        "\n",
        "    def r(self, s):\n",
        "        \"\"\"r(s) is used further down.\"\"\"\n",
        "        if self.m() > 0:\n",
        "            self.setto(s)\n",
        "\n",
        "    def step1ab(self):\n",
        "        \"\"\"step1ab() gets rid of plurals and -ed or -ing. e.g.\n",
        "\n",
        "           caresses  ->  caress\n",
        "           ponies    ->  poni\n",
        "           ties      ->  ti\n",
        "           caress    ->  caress\n",
        "           cats      ->  cat\n",
        "\n",
        "           feed      ->  feed\n",
        "           agreed    ->  agree\n",
        "           disabled  ->  disable\n",
        "\n",
        "           matting   ->  mat\n",
        "           mating    ->  mate\n",
        "           meeting   ->  meet\n",
        "           milling   ->  mill\n",
        "           messing   ->  mess\n",
        "\n",
        "           meetings  ->  meet\n",
        "        \"\"\"\n",
        "        if self.b[self.k] == 's':\n",
        "            if self.ends(\"sses\"):\n",
        "                self.k = self.k - 2\n",
        "            elif self.ends(\"ies\"):\n",
        "                self.setto(\"i\")\n",
        "            elif self.b[self.k - 1] != 's':\n",
        "                self.k = self.k - 1\n",
        "        if self.ends(\"eed\"):\n",
        "            if self.m() > 0:\n",
        "                self.k = self.k - 1\n",
        "        elif (self.ends(\"ed\") or self.ends(\"ing\")) and self.vowelinstem():\n",
        "            self.k = self.j\n",
        "            if self.ends(\"at\"):   self.setto(\"ate\")\n",
        "            elif self.ends(\"bl\"): self.setto(\"ble\")\n",
        "            elif self.ends(\"iz\"): self.setto(\"ize\")\n",
        "            elif self.doublec(self.k):\n",
        "                self.k = self.k - 1\n",
        "                ch = self.b[self.k]\n",
        "                if ch == 'l' or ch == 's' or ch == 'z':\n",
        "                    self.k = self.k + 1\n",
        "            elif (self.m() == 1 and self.cvc(self.k)):\n",
        "                self.setto(\"e\")\n",
        "\n",
        "    def step1c(self):\n",
        "        \"\"\"step1c() turns terminal y to i when there is another vowel in the stem.\"\"\"\n",
        "        if (self.ends(\"y\") and self.vowelinstem()):\n",
        "            self.b = self.b[:self.k] + 'i' + self.b[self.k+1:]\n",
        "\n",
        "    def step2(self):\n",
        "        \"\"\"step2() maps double suffices to single ones.\n",
        "        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n",
        "        string before the suffix must give m() > 0.\n",
        "        \"\"\"\n",
        "        if self.b[self.k - 1] == 'a':\n",
        "            if self.ends(\"ational\"):   self.r(\"ate\")\n",
        "            elif self.ends(\"tional\"):  self.r(\"tion\")\n",
        "        elif self.b[self.k - 1] == 'c':\n",
        "            if self.ends(\"enci\"):      self.r(\"ence\")\n",
        "            elif self.ends(\"anci\"):    self.r(\"ance\")\n",
        "        elif self.b[self.k - 1] == 'e':\n",
        "            if self.ends(\"izer\"):      self.r(\"ize\")\n",
        "        elif self.b[self.k - 1] == 'l':\n",
        "            if self.ends(\"bli\"):       self.r(\"ble\") # --DEPARTURE--\n",
        "            # To match the published algorithm, replace this phrase with\n",
        "            #   if self.ends(\"abli\"):      self.r(\"able\")\n",
        "            elif self.ends(\"alli\"):    self.r(\"al\")\n",
        "            elif self.ends(\"entli\"):   self.r(\"ent\")\n",
        "            elif self.ends(\"eli\"):     self.r(\"e\")\n",
        "            elif self.ends(\"ousli\"):   self.r(\"ous\")\n",
        "        elif self.b[self.k - 1] == 'o':\n",
        "            if self.ends(\"ization\"):   self.r(\"ize\")\n",
        "            elif self.ends(\"ation\"):   self.r(\"ate\")\n",
        "            elif self.ends(\"ator\"):    self.r(\"ate\")\n",
        "        elif self.b[self.k - 1] == 's':\n",
        "            if self.ends(\"alism\"):     self.r(\"al\")\n",
        "            elif self.ends(\"iveness\"): self.r(\"ive\")\n",
        "            elif self.ends(\"fulness\"): self.r(\"ful\")\n",
        "            elif self.ends(\"ousness\"): self.r(\"ous\")\n",
        "        elif self.b[self.k - 1] == 't':\n",
        "            if self.ends(\"aliti\"):     self.r(\"al\")\n",
        "            elif self.ends(\"iviti\"):   self.r(\"ive\")\n",
        "            elif self.ends(\"biliti\"):  self.r(\"ble\")\n",
        "        elif self.b[self.k - 1] == 'g': # --DEPARTURE--\n",
        "            if self.ends(\"logi\"):      self.r(\"log\")\n",
        "        # To match the published algorithm, delete this phrase\n",
        "\n",
        "    def step3(self):\n",
        "        \"\"\"step3() dels with -ic-, -full, -ness etc. similar strategy to step2.\"\"\"\n",
        "        if self.b[self.k] == 'e':\n",
        "            if self.ends(\"icate\"):     self.r(\"ic\")\n",
        "            elif self.ends(\"ative\"):   self.r(\"\")\n",
        "            elif self.ends(\"alize\"):   self.r(\"al\")\n",
        "        elif self.b[self.k] == 'i':\n",
        "            if self.ends(\"iciti\"):     self.r(\"ic\")\n",
        "        elif self.b[self.k] == 'l':\n",
        "            if self.ends(\"ical\"):      self.r(\"ic\")\n",
        "            elif self.ends(\"ful\"):     self.r(\"\")\n",
        "        elif self.b[self.k] == 's':\n",
        "            if self.ends(\"ness\"):      self.r(\"\")\n",
        "\n",
        "    def step4(self):\n",
        "        \"\"\"step4() takes off -ant, -ence etc., in context <c>vcvc<v>.\"\"\"\n",
        "        if self.b[self.k - 1] == 'a':\n",
        "            if self.ends(\"al\"): pass\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 'c':\n",
        "            if self.ends(\"ance\"): pass\n",
        "            elif self.ends(\"ence\"): pass\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 'e':\n",
        "            if self.ends(\"er\"): pass\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 'i':\n",
        "            if self.ends(\"ic\"): pass\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 'l':\n",
        "            if self.ends(\"able\"): pass\n",
        "            elif self.ends(\"ible\"): pass\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 'n':\n",
        "            if self.ends(\"ant\"): pass\n",
        "            elif self.ends(\"ement\"): pass\n",
        "            elif self.ends(\"ment\"): pass\n",
        "            elif self.ends(\"ent\"): pass\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 'o':\n",
        "            if self.ends(\"ion\") and (self.b[self.j] == 's' or self.b[self.j] == 't'): pass\n",
        "            elif self.ends(\"ou\"): pass\n",
        "            # takes care of -ous\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 's':\n",
        "            if self.ends(\"ism\"): pass\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 't':\n",
        "            if self.ends(\"ate\"): pass\n",
        "            elif self.ends(\"iti\"): pass\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 'u':\n",
        "            if self.ends(\"ous\"): pass\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 'v':\n",
        "            if self.ends(\"ive\"): pass\n",
        "            else: return\n",
        "        elif self.b[self.k - 1] == 'z':\n",
        "            if self.ends(\"ize\"): pass\n",
        "            else: return\n",
        "        else:\n",
        "            return\n",
        "        if self.m() > 1:\n",
        "            self.k = self.j\n",
        "\n",
        "    def step5(self):\n",
        "        \"\"\"step5() removes a final -e if m() > 1, and changes -ll to -l if\n",
        "        m() > 1.\n",
        "        \"\"\"\n",
        "        self.j = self.k\n",
        "        if self.b[self.k] == 'e':\n",
        "            a = self.m()\n",
        "            if a > 1 or (a == 1 and not self.cvc(self.k-1)):\n",
        "                self.k = self.k - 1\n",
        "        if self.b[self.k] == 'l' and self.doublec(self.k) and self.m() > 1:\n",
        "            self.k = self.k -1\n",
        "\n",
        "    def stem(self, p, i, j):\n",
        "        \"\"\"In stem(p,i,j), p is a char pointer, and the string to be stemmed\n",
        "        is from p[i] to p[j] inclusive. Typically i is zero and j is the\n",
        "        offset to the last character of a string, (p[j+1] == '\\0'). The\n",
        "        stemmer adjusts the characters p[i] ... p[j] and returns the new\n",
        "        end-point of the string, k. Stemming never increases word length, so\n",
        "        i <= k <= j. To turn the stemmer into a module, declare 'stem' as\n",
        "        extern, and delete the remainder of this file.\n",
        "        \"\"\"\n",
        "        # copy the parameters into statics\n",
        "        self.b = p\n",
        "        self.k = j\n",
        "        self.k0 = i\n",
        "        if self.k <= self.k0 + 1:\n",
        "            return self.b # --DEPARTURE--\n",
        "\n",
        "        # With this line, strings of length 1 or 2 don't go through the\n",
        "        # stemming process, although no mention is made of this in the\n",
        "        # published algorithm. Remove the line to match the published\n",
        "        # algorithm.\n",
        "\n",
        "        self.step1ab()\n",
        "        self.step1c()\n",
        "        self.step2()\n",
        "        self.step3()\n",
        "        self.step4()\n",
        "        self.step5()\n",
        "        return self.b[self.k0:self.k+1]"
      ],
      "metadata": {
        "id": "QUjIPj-BA99d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class DataProcessing:\n",
        "# Read Data\n",
        "# takes a file and reads the information and returns the target and strings\n",
        "def read_training_data(file_name, target_col = 0, string_col = 5):\n",
        "    f = open(file_name, 'r')\n",
        "    lines = f.readlines()\n",
        "\n",
        "    targets = []\n",
        "    strings = []\n",
        "    for line in lines:\n",
        "        line_parse = line.split(',')\n",
        "\n",
        "        for i in range(len(line_parse)):\n",
        "            line_parse[i] = line_parse[i].replace('\\\"', '')\n",
        "\n",
        "        targets.append(int(line_parse[target_col]))\n",
        "        strings.append(line_parse[string_col])\n",
        "\n",
        "    f.close()\n",
        "    return strings, targets\n",
        "\n",
        "# Preprocessor\n",
        "# takes the input string and lowers the case of the string\n",
        "# also removes @'s and links\n",
        "def preprocess(strings):\n",
        "    for i in range(len(strings)):\n",
        "        strings[i] = strings[i].lower()\n",
        "        strings[i] = re.sub(r'\\s+', ' ', re.sub(\n",
        "                r'[#\\^.,\\\"\\':;!?<>+=\\-~@$%&*\\(\\)\\{\\}\\[\\]\\\\\\/]*', ' ', re.sub(\n",
        "                    r'#[^.,\\\"\\':;!?<>+=\\-~@$%\\^&*\\(\\)\\{\\}\\[\\]\\\\\\/\\s]{0,280}\\b', ' ', re.sub(\n",
        "                        r'@[A-z0-9_]{3,15}\\b', ' ', re.sub(\n",
        "                            r'(https?:\\/\\/)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\\/=]*)\\b', ' ', strings[i]\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    return strings\n",
        "\n",
        "# Tokenizer\n",
        "# takes the input string and tokenizes sentences and single words\n",
        "def vocabulize(strings):\n",
        "    word_tok = []\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    for i, string in enumerate(strings):\n",
        "        stemmed_str = \"\"\n",
        "        words = string.split()\n",
        "        for word in words:\n",
        "            word = stemmer.stem(word, 0, len(word) - 1)\n",
        "            stemmed_str = stemmed_str + word\n",
        "            if not word in word_tok:\n",
        "                word_tok.append(word)\n",
        "\n",
        "        strings[i] = stemmed_str\n",
        "    word_tok = sorted(word_tok)\n",
        "\n",
        "    return word_tok, strings\n",
        "\n",
        "# One Hot Encoder\n",
        "# uses the bag of words to generate sentence vector encodings\n",
        "def hot_encode(strings, vocab):\n",
        "    encoded_list = []\n",
        "    str_encoding = [0] * len(vocab)\n",
        "\n",
        "    for string in strings:\n",
        "        for i in range(len(str_encoding)):\n",
        "            str_encoding[i] = 0\n",
        "        str_words = string.split()\n",
        "\n",
        "        for str_word in str_words:\n",
        "            for i, word in enumerate(vocab):\n",
        "                if str_word == word:\n",
        "                    str_encoding[i] += 1\n",
        "\n",
        "        encoded_list.append(str_encoding.copy())\n",
        "\n",
        "    return encoded_list"
      ],
      "metadata": {
        "id": "0wE26trOBEdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eop-0xYG86Dy"
      },
      "outputs": [],
      "source": [
        "# Nueral Net using sigmoid function\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(\n",
        "        self,\n",
        "        train = True,\n",
        "        gen_bow = True,\n",
        "        traing_data = \"./data/training_tweets.csv\",\n",
        "        learning_rate = .1,\n",
        "        hidden_layers = 3,\n",
        "        pickup = False,\n",
        "        update = False\n",
        "    ):\n",
        "        self.stemmer = stemmer = PorterStemmer()\n",
        "        self.alpha = learning_rate\n",
        "        self.update = update\n",
        "\n",
        "        if gen_bow == True:\n",
        "            if self.update:\n",
        "                print(\"[Progress] Reading training data\")\n",
        "            tweets, targets = read_training_data(traing_data) # list of tweets and list of targets\n",
        "            if self.update:\n",
        "                print(\"[Progress] Preprocessing the data\")\n",
        "            tweets = preprocess(tweets) # returns lowercase tweets and removes @users and links\n",
        "            if self.update:\n",
        "                print(\"[Progress] Generating bag of words\")\n",
        "            self.bag_of_words, tweets = vocabulize(tweets) # generates bag-of-words\n",
        "            if self.update:\n",
        "                print(\"[Progress] Saving bag of words\")\n",
        "            self._save_bag_of_words()\n",
        "        else:\n",
        "            if self.update:\n",
        "                print(\"[Progress] Loading bag of words\")\n",
        "            self._load_bag_of_words()\n",
        "\n",
        "        if train == True:\n",
        "            if self.update:\n",
        "                print(\"[Progress] Encoding training data\")\n",
        "            tweets_encoded = hot_encode(tweets, self.bag_of_words)\n",
        "            if self.update:\n",
        "                print(\"[Progress] Generating layer sizes\")\n",
        "            self.layer_sizes = [len(self.bag_of_words)] + [10] * hidden_layers + [1]\n",
        "\n",
        "            if not pickup:\n",
        "                if self.update:\n",
        "                    print(\"[Progress] Generating weights\")\n",
        "                self._generate_weights()\n",
        "                if self.update:\n",
        "                    print(\"[Progress] Generating biases\")\n",
        "                self._generate_biases()\n",
        "            else:\n",
        "                if self.update:\n",
        "                    print(\"[Progress] Loading weights\")\n",
        "                self._load_weights()\n",
        "                if self.update:\n",
        "                    print(\"[Progress] Loading biases\")\n",
        "                self._load_biases()\n",
        "\n",
        "            if self.update:\n",
        "                print(\"[Progress] Commencing training\")\n",
        "            self._train(tweets_encoded, targets, len(tweets_encoded))\n",
        "        else:\n",
        "            if self.update:\n",
        "                print(\"[Progress] Generating layer sizes\")\n",
        "            self.layer_sizes = [len(self.bag_of_words)] + [10] * hidden_layers + [1]\n",
        "            if self.update:\n",
        "                print(\"[Progress] Loading weights\")\n",
        "            self._load_weights()\n",
        "            if self.update:\n",
        "                print(\"[Progress] Loading biases\")\n",
        "            self._load_biases()\n",
        "\n",
        "    def __call__(self, text = None, input_vector = None):\n",
        "        if text != None:\n",
        "            text = preprocess([text])[0]\n",
        "            stemmed_str = \"\"\n",
        "            words = text.split()\n",
        "            for word in words:\n",
        "                word = self.stemmer.stem(word, 0, len(word) - 1)\n",
        "                stemmed_str = stemmed_str + word\n",
        "            input_vector = hot_encode([stemmed_str], self.bag_of_words)[0]\n",
        "\n",
        "        z_values = self.biases.copy()\n",
        "        for l in range(0, len(self.layer_sizes) - 1):\n",
        "            for i in range(self.layer_sizes[l + 1]):\n",
        "                if l == 0:\n",
        "                    z_values[l][i] = self._sigmoid(\n",
        "                        np.dot(input_vector, self.weights[l][i]) + self.biases[l][i]\n",
        "                    )\n",
        "                else:\n",
        "                    z_values[l][i] = self._sigmoid(\n",
        "                        np.dot(z_values[l - 1], self.weights[l][i]) + self.biases[l][i]\n",
        "                    )\n",
        "        return z_values[len(z_values) - 1][0]\n",
        "\n",
        "    def _generate_weights(self):\n",
        "        self.weights = []\n",
        "        for l in range(1, len(self.layer_sizes)):\n",
        "            temp_weight_matrix = []\n",
        "            for size_of_l in range(self.layer_sizes[l]):\n",
        "                temp_weight_vector = []\n",
        "                for size_of_prev_l in range(self.layer_sizes[l - 1]):\n",
        "                    temp_weight_vector.append(\n",
        "                        np.random.uniform(-10, 11)\n",
        "                    )\n",
        "                temp_weight_matrix.append(temp_weight_vector)\n",
        "            self.weights.append(temp_weight_matrix)\n",
        "\n",
        "    def _generate_biases(self):\n",
        "        self.biases = []\n",
        "        for l in range(1, len(self.layer_sizes)):\n",
        "            temp_bias_vector = [0] * self.layer_sizes[l]\n",
        "            self.biases.append(temp_bias_vector)\n",
        "\n",
        "    def _save_bag_of_words(self):\n",
        "        f = open(\"./data/bag_of_words.dat\", 'w')\n",
        "        for word in self.bag_of_words:\n",
        "            f.write(word + '\\n')\n",
        "        f.close()\n",
        "\n",
        "    def _save_weights(self):\n",
        "        f = open(\"./data/weights.dat\", 'w')\n",
        "        for i, weight_matrix in enumerate(self.weights):\n",
        "            for weight_vector in weight_matrix:\n",
        "                for i, weight in enumerate(weight_vector):\n",
        "                    if i != (len(weight_vector) - 1):\n",
        "                        f.write(str(weight) + \",\")\n",
        "                    else:\n",
        "                        f.write(str(weight) + \"\\n\")\n",
        "        f.close()\n",
        "\n",
        "    def _save_biases(self):\n",
        "        f = open(\"./data/biases.dat\", 'w')\n",
        "        for bias_vector in self.biases:\n",
        "            for i, bias in enumerate(bias_vector):\n",
        "                if i != (len(bias_vector) - 1):\n",
        "                    f.write(str(bias) + \",\")\n",
        "                else:\n",
        "                    f.write(str(bias) + \"\\n\")\n",
        "        f.close()\n",
        "\n",
        "    def _load_bag_of_words(self):\n",
        "        f = open(\"./data/bag_of_words.dat\", 'r')\n",
        "        self.bag_of_words = f.readlines();\n",
        "        f.close()\n",
        "\n",
        "    def _load_weights(self):\n",
        "        f = open(\"./data/weights.dat\", 'r')\n",
        "        self.weights = []\n",
        "        for l in range(1, len(self.layer_sizes)):\n",
        "            temp_weight_matrix = []\n",
        "            for size_of_l in range(self.layer_sizes[l]):\n",
        "                temp_weight_vector = []\n",
        "                for weight in f.readline().split(','):\n",
        "                    temp_weight_vector.append(float(weight))\n",
        "                temp_weight_matrix.append(temp_weight_vector)\n",
        "            self.weights.append(temp_weight_matrix)\n",
        "\n",
        "    def _load_biases(self):\n",
        "        f = open(\"./data/biases.dat\", 'r')\n",
        "        self.biases = []\n",
        "        for l in range(1, len(self.layer_sizes)):\n",
        "            temp_bias_vector = []\n",
        "            for bias in f.readline().split(','):\n",
        "                temp_bias_vector.append(float(bias))\n",
        "            self.biases.append(temp_bias_vector)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def _sigmoid_deriv(self, x):\n",
        "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
        "\n",
        "    def _update_parameters(self, input_vector, target):\n",
        "        # Forward propigate\n",
        "        # Dot Product into Sigmoid\n",
        "        #   We need Z values\n",
        "        z_values = self.biases.copy()\n",
        "        for l in range(0, len(self.layer_sizes) - 1):\n",
        "            for i in range(self.layer_sizes[l + 1]):\n",
        "                if l == 0:\n",
        "                    z_values[l][i] = self._sigmoid(\n",
        "                        np.dot(input_vector, self.weights[l][i]) + self.biases[l][i]\n",
        "                    )\n",
        "                else:\n",
        "                    z_values[l][i] = self._sigmoid(\n",
        "                        np.dot(z_values[l - 1], self.weights[l][i]) + self.biases[l][i]\n",
        "                    )\n",
        "        # Back propigate\n",
        "        # Delta calculation into Weight Correction\n",
        "        #   We need bias and weight\n",
        "        d_weights = z_values.copy()\n",
        "        d_biases = z_values.copy()\n",
        "        d_weights[len(self.layer_sizes) - 2][0] = target - z_values[len(self.layer_sizes) - 2][0]\n",
        "        d_biases[len(self.layer_sizes) - 2][0] = target - z_values[len(self.layer_sizes) - 2][0]\n",
        "        for l in range(len(d_weights) - 2, -1, -1):\n",
        "            for i in range(len(d_weights[l])):\n",
        "                d_weights[l][i] = 0\n",
        "                d_biases[l][i] = 0\n",
        "                for j in range(len(d_weights[l + 1])):\n",
        "                    d_weights[l][i] = d_weights[l][i] + self.weights[l + 1][j][i] * d_weights[l + 1][j]\n",
        "                    d_biases[l][i] = d_biases[l][i] + self.biases[l + 1][j] * d_biases[l + 1][j]\n",
        "                d_weights[l][i] = z_values[l][i] * d_weights[l][i] * self._sigmoid_deriv(z_values[l][i])\n",
        "                d_biases[l][i] = z_values[l][i] * d_biases[l][i] * self._sigmoid_deriv(z_values[l][i])\n",
        "\n",
        "        for l in range(len(d_weights)):\n",
        "            for i in range(len(d_weights[l])):\n",
        "                self.biases[l][i] = self.biases[l][i] - self.alpha * d_biases[l][i]\n",
        "                for j in range(len(self.weights[l][i])):\n",
        "                    self.weights[l][i][j] = self.weights[l][i][j] - self.alpha * d_weights[l][i]\n",
        "\n",
        "    def _train(self, input_vectors, targets, iterations = 1000):\n",
        "        cumulative_errors = []\n",
        "        for current_iteration in range(iterations):\n",
        "            # Pick a data instance at random\n",
        "            random_pos = np.random.randint(len(input_vectors))\n",
        "\n",
        "            # Compute the gradients and update the weights\n",
        "            self._update_parameters(input_vectors[random_pos], targets[random_pos])\n",
        "\n",
        "            # Measure the cumulative error for all the instances\n",
        "            if current_iteration % 100 == 0:\n",
        "                cumulative_error = 0\n",
        "                # Loop through all the instances to measure the error\n",
        "                for i in range(len(input_vectors)):\n",
        "                    cumulative_error = cumulative_error + np.square(\n",
        "                        targets[i] - self(input_vector = input_vectors[i])\n",
        "                    )\n",
        "                cumulative_errors.append(cumulative_error)\n",
        "                if self.update:\n",
        "                    print(f\"[Progress] Cumulative error is {cumulative_error} at iteration {current_iteration}\")\n",
        "                    print(\"[Progress] Saving weights\")\n",
        "                self._save_weights()\n",
        "                if self.update:\n",
        "                    print(\"[Progress] Loading biases\")\n",
        "                self._save_biases()\n",
        "\n",
        "        return cumulative_errors"
      ]
    }
  ]
}